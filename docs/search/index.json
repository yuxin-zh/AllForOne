[{"content":"","date":"2022-02-16T00:55:19+08:00","image":"https://yuxin-zh.github.io/AllForOne/p/spring/b5_hu896e223e610358494ca46474686101a7_201319_120x120_fill_q75_box_smart1.jpg","permalink":"https://yuxin-zh.github.io/AllForOne/p/spring/","title":"Spring"},{"content":"[TOC]\nLINUX安装与配置 1.虚拟机安装  新建虚拟机，选择自定义    选择默认版，然后下一步\n  然后点击稍后安装操作系统\n  选择centos 7 64位\n  更改安装位置到你希望安装的位置\n  以下电脑配置高的可以选择配置多个处理器\n  虚拟机内存默认是1G，如果有需要可按需调配，这里配置4G\n  网络类型选择使用仅主机模式网络\n  SCSI控制器选择LSI Logic（默认）\n  磁盘选择创建新的\n  最大磁盘大小选择默认的20G\n  完成安装\n  2.安装镜像   点击编辑虚拟机设置\n  点击CD/DVD\n  点击使用ISO镜像文件，然后选择安装的cent的镜像\n  开启虚拟机，选择install centos 7 回车\n  安装结束后选择中文\n  点击软件选择\n  这里我们选择最小安装，你也可以根据自己的需要选择\n  然后是点击安装目标位置，直接 点击完成即可\n  再点击网络和主机名，点击配置\n  点击ipv4设置，讲虚拟机设置静态ip，ip地址为192.168.xx.xx,注意将DNS服务器和网关设置一致，子网掩码设置为255.255.255.0\n  点击ipv6 切换为忽略，然后完成。\n  点击完成\n  设置主机名，点击应用\n  完成\n  设置root密码\n  设置完成后，重启。\n  可能需要等待一段时间\n  输入密码，登录成功 登陆时用户名为root，密码是你设置的那个密码\n  修改网络配置\n1  vi/etc/sysconfig/network-scripts/ifcfg-ens33     修改如下配置\n1 2 3 4  IPADDR=\u0026#34;192.168.50.XX(这里的数字可以自己选择)\u0026#34; //IP地址 PREFIX=\u0026#34;24\u0026#34; GATEWAY=\u0026#34;192.168.50.2\u0026#34;//网关 DNS1=\u0026#34;192.168.50.2 //DNS服务器\u0026#34;     然后点击service network restart\n  查看ip地址ifconfig,这样你的虚拟机就安装完成了\n  3.配置虚拟机网络适配器   点击网络适配器\n  点击编辑\n  然后编辑ip地址信息vi /etc/sysconfig/network-scripts/ifcfg-ens33\n1 2 3 4  然后修改BOOTPROTO=static ONBOOY=yes 加上DNS2=8.8.8.8 然后按下ESC 输入\u0026#34;:wq\u0026#34; 回车     保存后输入systemctl restart network或service network restart\n  4.配置DNS   一次输入以下命令：\ncd /etc/NetworkManager\nvi NetworkManager.conf再最后添加dns=none\n  然后vi /etc/resolv.conf\n    输入你之前保存的两个NDS\n  尝试ping www.baidu.com 内网\n  ping 172.16.50.171 外网\n  ","date":"2022-02-16T00:54:24+08:00","image":"https://yuxin-zh.github.io/AllForOne/p/linux/b4_hufc4426c01aefee65074e77ba9d297e0e_200631_120x120_fill_q75_box_smart1.jpg","permalink":"https://yuxin-zh.github.io/AllForOne/p/linux/","title":"Linux"},{"content":"1  \u0026lt;a href=https://www.jianshu.com/p/bcc54f63abe4\u0026gt;\u0026lt;/a\u0026gt;   [toc]\nHbase 1.Overview Hbase  是一种分布式 可拓展 支持海量数据存储的NoSQL数据库 hdfs不支持随机写，基于HDFS，hbase可以实现随时写 hadoop和habase都是存储大量数据的 但不同的是hadoop的分布式文件系统中，数据分布在网络的不同节点中 而hbase是个数据库 列行的形式存储  1.1数据模型   NameSpace：类似于关系数据库中的database概念，每个ns下有多个表。Hbase自带两个ns：hbase和default.hbase。 hbase存放的是HBase内置的表，default表是用户默认使用的命名空间。\n  Region：表的切片 。刚开始创建就一个region\n  Row：Hbase表中的每行数据都由一个Rowkey和多个列组成，数据是按照Rowkey的字典序进行存储的，并且查询时只能根据rowkey进行查询\n  Column：hbase每个列都由Column Family和Column Qualifier（列限定符）进行限定\n  TimeStamp：用于标识数据的不同版本，数据仔写入时，不指定的话，系统会自动为其加上该字段，值为写入hbase的时间\n  Cell：由{Rowkey，clolumn Family，clolumn Qualifier，timeStamp}唯一确定的单元，cell中的数据是没有类型的，全部是字节码的形式存在。\n  1.2 Hbase基本架构  Store\u0026ndash;\u0026gt;Region\u0026ndash;\u0026gt;Region Server(分布式服务)  RegionServer：DML 对表中数据进行操作  Data：get，put（提交新数据覆盖元数据，而不是修改元数据），delete Region：splitRegion，compactRegion 切分和合并     Master挂掉的话，数据的增删改查仍可以，但是做表结构的不行 master管理ddl  Master：DDL 对表进行操作  Table：create,delete,alter 关于表进行操作 RegionServer：分配Regions到每个Regionserver，监控每个RegionServer的status   Zookeeper：HBase通过Zookeeper来做master的高可用、RegionServer的监控，元数据的入口以及集群配置的维护等工作    1.3 Hbase的安装和配置 1.4 HBase Shell操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73  #general status： #ddl alter： 1）add 列族到表中 alter \u0026#39;表名\u0026#39;，NAME=\u0026gt;\u0026#39;列族名\u0026#39;，VERSIONS=\u0026gt;5 alter \u0026#39;t1\u0026#39;, \u0026#39;f1\u0026#39;, {NAME =\u0026gt; \u0026#39;f2\u0026#39;, IN_MEMORY =\u0026gt; true}, {NAME =\u0026gt; \u0026#39;f3\u0026#39;, VERSIONS =\u0026gt; 5} 2) delte 列族 alter \u0026#34;ns:表名\u0026#34;,NAME=\u0026gt;\u0026#39;列族名\u0026#39;，METHOD=\u0026gt;\u0026#39;delete\u0026#39; alter \u0026#39;ns:表名\u0026#39;,\u0026#39;delete\u0026#39;=\u0026gt;\u0026#39;f1\u0026#39; 3)change table-scope的属性 like below alter \u0026#39;table name\u0026#39;,MAX_FILESIZE=\u0026gt;\u0026#39;134217728\u0026#39; 4)设置coprocessor 不知道是否仔niit范围内，可以课下学学 5) 通过hbase命令行设置habse的配置信息 6) set Regin_Replication alter \u0026#39;table_name\u0026#39;,{REGION_REPLICATION=\u0026gt;2} 7)disable/enable table split or merge alter \u0026#39;tn\u0026#39;,{SPLIT_ENABLED=\u0026gt;false} alter \u0026#39;tn\u0026#39;,{MERGE_ENABLED=\u0026gt;false} alter_status:#Get the status of the alter command. Indicates the number of regions of thetable that have received the updated schema ,Passed by table name. create: create \u0026#39;ns:tn\u0026#39;,{NAME=\u0026gt;\u0026#39;\u0026#39;,VERSIONS=\u0026gt;},{},{} desc \u0026#39;ns;tn\u0026#39; disable: disable \u0026#39;ns:tn\u0026#39; disable_all drop: should be executed after \u0026#34;disable\u0026#34; drop \u0026#39;ns:tn\u0026#39; drop_all enable: enable \u0026#39;ns:tn\u0026#39; enable_all exits:判断表是否存在 exists \u0026#39;ns;tb\u0026#39; get_table: #Get the given table name and return it as an actual object to be manipulated by the user #namespacelis create_namespace \u0026#39;ns1\u0026#39; describe_namespace \u0026#39;ns1\u0026#39; desc不行 drop_namespace \u0026#39;\u0026#39; list_namespace #DML append \u0026#39;t1\u0026#39;,\u0026#39;r1\u0026#39;,\u0026#39;c1\u0026#39;,\u0026#39;value\u0026#39;相当于String的append value加在原有value的后 count: 计数一个表中的row数量 count \u0026#39;tn\u0026#39; 或者建立一个t-\u0026gt;\u0026#39;tn\u0026#39;的reference t.count也可以 #####Get \u0026amp;\u0026amp;\u0026amp;\u0026amp; Scan get的最大范围是指定到rowkey，甚至还可以指定到时间戳 #利用scan查看同一个cell之前已经put的数据（scan时可以设置是否开启RAW模式，开启RAW模式会返回已添加删除标记但是未实际进行删除的数据） #get获取某个cell保留的（未添加删除标记）的所有version数据（在describe 表名，查看列族VERSIONS是多少，get就会多少数据(cell的数据大于等于VERSIONS的数量)）   1.5Hbase的读写流程 对于hbase来说 读比写要慢\n 写流程  like put的流程：首先client会\u0026mdash;-请求meta表所在的RegionServer\u0026mdash;\u0026gt; ZooKeeper\u0026mdash;-meta:hadoop02\u0026mdash;\u0026ndash;\u0026gt;client meta表存的就是元数据 然后client\u0026mdash;请求meta\u0026mdash;-\u0026gt;hadoop02\u0026mdash;\u0026mdash;返回meta 获取RS\u0026mdash;\u0026ndash;\u0026gt;client\u0026mdash;\u0026ndash;元数据存入缓存\u0026mdash;\u0026ndash;\u0026gt;meta cache(下次查找会先查询缓存，不存在则请求ZK) 找到后client\u0026mdash;发送Put请求\u0026mdash;-\u0026gt;RegionServer\u0026mdash;\u0026gt;wal\u0026mdash;-\u0026gt;memstore\u0026mdash;\u0026ndash;\u0026gt;client 注意windows时间与linux时间未统一的话，不要传时间戳参数    1.6Hbase特性  容量大 面向列 稀疏性：null的空列不占用存储空间 拓展性：Hbase可以动态增加regionserver 高性能：LSM数据结构  1.7应用场景  搜索引擎 增量数据存储 OPENTSDB 捕捉用户交互数据 广告效果和点击流 内容服务 信息交换  1.8Hbase与关系型数据库的区别  HBase 里面有以下 2 个主要概念：  Rowkey: HBase 中的记录是按照 rowkey 来排序的； Column family：(列族)是在系统启动之前预先定义好的；   HBase 优缺点：  不支持条件查询以及 orderby 等查询； 列可以动态增加，列为空则不存储数据，节省存储空间； 会自动切分数据； 可以提供高并发读写操作的支持；   注意事项  Row key 行键 (Row key)可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10- 100bytes)，在 hbase 内部，row key 保存为字节数组。 列族是表的 schema 的一部分(而列不是)，必须在使用表 之前定义。 HBase 中通过 row 和 columns 确定的为一个存贮单元称为 cell。每个 cell 都保存着同一份数据的 多个版本 cell 中的数据是没有 类型的，全部是字节码形式存贮。 HBASE 强依赖 hadoop 的 hdfs 系统，hbase 的版本需要和 hadoop 的版本匹配，否则会出现一些意 外情况    1.9hbase的启动命令  启动单机模式：bin/start-hbase.sh 查看进程：jps 启动集群：start-hbase。sh  2.Hbase基础 2.1 Hbase表的物理属性  Table 在行的方向上分割为多个 Region； Region 按大小分割的，每个表开始只有一个 region，随着数据增多，region 不断增 大，当增大到一个阀值的时候，region 就会等分会两个新的 region，之后会有越来 越多的 region； Region 是 Hbase 中分布式存储和负载均衡的最小单元，不同 Region 分布到不同 RegionServer 上。  2.2Region   Region组成\n Region 由一个或者多个 Store 组成，每个 store 保存一个 columns family； 每个 Strore 又由一 个 memStore 和 0 至多个 StoreFile 组成，StoreFile 包含 HFile；memStore 存储在内存中， StoreFile 存储在 HDFS 上 Region 虽然是分布式存储的最小单元，但并不是存储的最小单元，Region 默认大小是 256M，当一个 Region 大小超过设置的值，Hmaster 会将 Region 拆分为 2 个子 Region，同时父 Region 下线。    RegionServer(DML)\n Region server 维护 Master 分配给它的 region，处理对这些 region 的 IO 请求  Region server 负责切分在运行过程中变得过大的 regio    Hmaster\n 管理用户对 Table 的增、删、改、查操作  管理 RegionServer 的负载均衡、调整 Region 的分布  在 Region Split 后，将新 Region 分布到不同的RegionServer。  在 RegionServer 宕机后，那该 RegionServer 上所管理的 Region 由 HMaster 进行重新分 配。    管理类\n  HbaseAdmin：提供接口关系 HBase 数据库中的表信息\n  1 2  HBaseAdmin admin = new HBaseAdmin(config);     HTableDescriptor:维护表的信息\n  1 2 3  setMaxFileSize，指定最大的 region size setMemStoreFlushSize 指定 memstore flush 到 HDFS 上的文件大小 通过 addFamily 方法增加 famil     HColumnDescriptor: 代表的是 column 的 schema，\n  1 2 3 4 5 6 7 8 9 10  setTimeToLive:指定最大的 TTL,单位是 ms,过期数据会被自动删除。  setInMemory:指定是否放在内存中，对小表有用，可用于提高效率。默认关闭  setBloomFilter:指定是否使用 BloomFilter,可提高随机查询效率。默认关闭  setCompressionType:设定数据压缩类型。默认无压缩。  setMaxVersions:指定数据最大保存的版本个数。默认为 3。 HTableDescriptor htd =newHTableDescriptor(tablename); htd.addFamily(new HColumnDescriptor(“myFamily”));       3. 1.HBase的存储逻辑  HBase 的表中的数据分隔是使用列族而不是列。每个列族对应一个 HFile 文件，在 HDFS 上以一个 目录的形式存放。 Key-Value存储模型：  key是RowKey，Value是列族的集合。\u0026mdash;\u0026gt;也就是说我们可以通过rowkey检索到value 行键的设计：、  表扫描是对行键的操作，所以，行键的设计控制着你能够通过 HBase 执行的实时/直接获取量。 当在生产环境中运行 HBase 时，它在 HDFS 上部运行，数据基于行键通过 HDFS，如果你所有的 行键都是以 user-开头，那么很有可能你大部分数据都被分配一个节点上（违背了分布式数据 的初衷），因此，你的行键应该是有足够的差异性以便分布式地通过整个部署。     在Hbase中表可以设计为高表(tall-narrow table)和宽表(flat-wide table)的形式 高表指的是列少行多 宽表反之  ","date":"2022-02-16T00:53:15+08:00","image":"https://yuxin-zh.github.io/AllForOne/p/hbase/b3_hu5e6f55736be3b54e76a3b17fa38b56ba_71449_120x120_fill_q75_box_smart1.jpg","permalink":"https://yuxin-zh.github.io/AllForOne/p/hbase/","title":"Hbase"},{"content":"[toc]\nHadoop相关概念 BIg Data\n1 2 3 4  Volume Velocity（快速） Variety (结构化数据和非结构化数据) Value(低价值密度)：价值密度的大小与数据总量的大小成反比   HIVE：存储 查询和分析存储在hdfs上的大量数据\n缺点 ：不支持事务，不可以修改数据，只可以通过文件追加和重新上传 速度很慢\nZOOkeeper“zn+1哥服务器允许n此错误。\n结构化数据\n1  数据库这种有二维表格的   半结构化数据\n1  类似于一个文件，但是可以导入mysql这种结构化数据中   非结构化数据\n1  无法转化，视频，ppt等   Hadoop\n1 2  分布式系统基础架构：多台服务器共同完成某一任务 主要解决海量数据的存储和分析计算   三大发行版本\n1 2 3  Apache 最基础 Cloudera Horntownworks   Hadoop优势\n1 2 3 4  高可靠性：底层维护多个数据副本，即使某个元素或存储出现故障，data-loss is avoided 高扩展性：在集群间分配任务数据，可方便扩展节点，动态增加和动态删除 高效性：hadoop是并行工作的 高容错性：能够自动将失败任务重新分配   hadoop组成（important）\n1 2 3 4 5 6 7 8  Hadoop 1.x: HDFS:数据存储 Mapduce：计算+资源调度 Common：辅助工具 Hadoop 2.x: 多了Yarn用于资源调度 Hadoop 3.x： 在组成上没有区别   HDFS数据存储\n1 2 3 4 5 6 7 8  NameNode:存储文件的元数据，如文件名，文件目录结构，文件属性等 保存在linux中 DataNode：存储文件块数据，以及快数据的校验和{ 文件块：最基本的存储单位 HDFS默认的block大小是64MB（老版本) 不同于普通文件系统的是，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间 Replication：多复本 默认是三个，可通过配置文件配置 } 2NN:每隔一段时间对NameNode元数据备份,只能恢复一部分数据而非所有   关于replication的解析：Yarn架构\n1 2 3 4 5 6 7 8 9 10 11 12 13  Resource Manager:管理整个集群资源 rm将资源部份安排给基础大的Node Manager { *NM遵循来自RM的一些指令来管理单一节点上的可用资源 *AM负责与RM协商资源并于NM合作启动容器 } Node Manager:管理单个节点的服务器资源 是yarn中每个节点上的代理 与RM通讯 监督container的生命周期 ApplicationMaster：单个运行任务的boss 流程如下 和rm协商，获取资源 通过rm来获取任务 和NM启动任务 Map或Reduce ---------------\tContainer：容器，相当于于一台独立服务器，封装任务运行需要的资源 封装的是某个DataNode节点上的资源 AppMaster请求资源时，RM以container的形式返回资源 Scheduler:资源调度器根据队列容量，队列限制，为每个应用分配一定的资源。（只是单纯的资源调度，不参与任何任务状态管理）   MapReduce架构\n1  计算分为两个阶段：Map 和Reduce   HDFS\u0026amp;Yarn\u0026amp;MapReduce\n1  remain to be done   Hadoop运行模式\n1 2 3  Local:数据存储再linux本地，从本地读取 ~测试偶尔会用 伪分布式（pseudo-distributed）：数据存储在HDFS ~公司比较差钱 完全分布式（fully-distributed）:数据也是存储在HDFS，但是多台服务器工作 ~大量使用   tip\n1  后续写的mapreduce程序必须指定对应的输入路径和输出路径，而且输出路径还不能存在，存在的话会直接抛出异常。 Get it!   Hadoop官方文档：FileSystem (Apache Hadoop Main 3.3.1 API)\nHDFS Introduction 分布式文件系统，建立在一次写入，多次读取的思想上。\n一个hdfs集群是由一个namenode和多个datanode形成，内部机制是将一个文件分割成一个或多个的block，这些块储存在一组数据节点中。\nnamenode负责文件或目录的“打开，关闭，重命名等”,并确定块与数据节点的映射。而数据节点负责来自文件系统客户的读写请求\nHDFS 块  默认的最基本的存储单位是64M的数据块**（默认大小在hadoop2.x版本中是128M，老版本中是64M）** HDFS的文件是被分成64M一块的数据块进行存储的 HDFS中若一个文件大小小于64M，并不占用整个存储空间  1）hadoop block存放策略  第一个block放在client所在的node里面 第二个放在与第一个不同的机架中的node中 第三个放在与第一个块的同一个机架的不用node中  2）NameNode和DataNode  NameNode(元数据节点)：用来管理文件系统的命名空间  将所有文件和文件夹的元数据保存在一个system tree上（ 元数据 ：指用来描述一个 文件 的特征的系统数据，诸如访问权限、 文件 拥有者、以及 文件 数据块的分布信息等等） 也同时会在硬盘上保存成这些文件：namespace image 和editlog 也存储了一个文件包含哪些数据块，分布在哪些数据节点上 注意：这些信息并不真正存储在硬盘 而是存放在数据节点中 client和namenode可以向数据节点请求写入或者读出数据块，并周期性的向namenode汇报其存储的数据块信息 元数据节点目录结构：在hdfs-site.xml中配置dfs.name.dir参数    1 2 3 4 5  namespaceID=123214214 文件系统的唯一标识符 在文件系统初始化时生成的 ClusterID=xxxxxxxxx 系统生成或手动指定的集群ID，可以在-clusteid选项中使用 cTime=0 0此处标识namenode的创建时间，更新nd后会更新时间戳 storageType=NAME_NODE 表示此文件夹中保存的是元数据节点的数据结构 LayoutVersion 保存格式版本号   checkpoint的作用：1）保证数据库的一致性，这是指将脏数据写入到硬盘，保证内存和硬盘上的数据是一样的;2）缩短实例恢复的时间\n3）HDFS通信协议 4）HDFS的安全模式 5）客户端读文件 1 2 3 4  //工具类获取FileSystem  String uri = \u0026#34;域名\u0026#34;; //like \u0026#34;hdfs://niit1:9000\u0026#34;  Configuration conf = new Configuration(); FileSystem fs= FileSystem.get(URI.create(uri),conf,\u0026#34;你的用户名,也可以不加，区别建议查阅官方API\u0026#34;);   1 2 3 4 5 6 7 8 9 10  String uri = \u0026#34;hdfs://niit1:9000\u0026#34;; Configuration conf = new Configuration(); FileSystem fs= FileSystem.get(URI.create(uri),conf); byte[] file_content_buff = \u0026#34;hello hadoop world,test write file! \\n\u0026#34;.getBytes();//以字节数组的形式读取字符串  Path dfs = new Path(\u0026#34;/user/root/niit/nn.txt\u0026#34;); //写入文件路径  FSDataOutputStream outputStream = fs.create(dfs); //create file两种形式 1）使用setpermission提供的权限来设置权限 2）将配置中的umask改为0---\u0026gt;但并非线程安全的  outputStream.write(file_content_buff,0,file_content_buff.length);//写入文件  fs.close();      static FileSystem get(URI uri, Configuration conf)Get a FileSystem for this URI\u0026rsquo;s scheme and authority.     static FileSystem get(URI uri, Configuration conf, String user)Get a FileSystem instance based on the uri, the passed in configuration and the user.    6）客户端写文件 1 2 3 4 5 6 7 8 9 10 11  FileSystem fs = (new GetFileSystem()).getSystem(); //GetFileSystem是上面的工具类 用来获取FileSystem  InputStream in =null; try { //Opens an FSDataInputStream at the indicated Path.  in = fs.open(new Path(\u0026#34;/user/root/niit/nn1.txt\u0026#34;)); IOUtils.copyBytes(in,System.out,4096,false); //(InputStream in, OutputStream out, int buffSize, boolean close)  } finally { IOUtils.closeStream(in); }   7）本地文件上传到HDFS 使用 copyFromLocalFile接口\n1 2 3 4 5 6  public static void main(String[] args) throws IOException { FileSystem fs = (new GetFileSystem()).getSystem(); Path src = new Path(\u0026#34;本地文件路径\u0026#34;); Path desc = new Path(\u0026#34;目标路径\u0026#34;); fs.copyFromLocalFile(src,desc); }   8）重命名文件 使用rename（抽象类）返回布尔值\n1 2 3 4 5 6  FileSystem fs = (new GetFileSystem()).getSystem(); //已经在classpath下面加载了hdfs-site.xml和core-site.xml的配置文件，则会自动理解输入的文件路径为hdfs的  Path CurrentName = new Path(\u0026#34;当前文件路径及文件名\u0026#34;); Path desName = new Path(\u0026#34;目标文件名\u0026#34;); //是否可以同时修改文件路径?YESSSSSSS!!!!  boolean result = fs.rename(CurrentName,desName); System.out.println(result==true?\u0026#34;修改成功\u0026#34;:\u0026#34;修改失败\u0026#34;);   9）Delete File \u0026amp; Directory 1 2 3 4  FileSystem fs = (new GetFileSystem()).getSystem(); Path delef = new Path(\u0026#34;删除的目标地址\u0026#34;); boolean deleRe = fs.delete(delef,true); System.out.println(deleRe==true? \u0026#34;yes\u0026#34;:\u0026#34;no\u0026#34;);   10）创建目录和遍历目录 创建过程和删除相似 无非是使用了mkdirs接口罢了\n遍历目录\n1 2 3 4 5 6 7 8 9  public static void main(String[] args) throws IOException { FileSystem fs = (new GetFileSystem()).getSystem(); FileStatus[] fsstatus = fs.listStatus(new Path(\u0026#34;/user/root\u0026#34;)); for(FileStatus status : fsstatus) if(status.isFile()) System.out.println(\u0026#34;file:\u0026#34;+status.getPath().toString()); else System.out.println(\u0026#34;directory:\u0026#34;+status.getPath().toString()); }      abstract FileStatus[] listStatus(Path f)List the statuses of the files/directories in the given path if the path is a directory.     FileStatus[] listStatus(Path[] files)Filter files/directories in the given list of paths using default path filter.   FileStatus[] listStatus(Path[] files, PathFilter filter)Filter files/directories in the given list of paths using user-supplied path filter.   FileStatus[] listStatus(Path f, PathFilter filter)Filter files/directories in the given path using the user-supplied path filter.    HadoopShell Commands 1  阿巴阿巴   Hadoop IO 1）序列化 序列化：将结构化对象转化为字节流以便于通过网络进行传输或写入持久存储的过程 反序列化则反之\n  特点：\n  Compact: 方便网络传输\n  Fast：性能好\n  Extensible：协议有变化 可以支持新的需求（动态）\n  Interoperable：客户端和服务器端不依赖语言的实现（怎么去理解）\n  hadoop使用writables，满足compact fast 但是不满足拓展性 （这种序列化时hadoop自己实现的，而不是java的序列化）\n1 2 3 4 5 6 7 8 9 10  public interface Serialization\u0026lt;T\u0026gt; { //客户端用于判断序列化实现是否支持该类对象  boolean accept(Class\u0026lt;?\u0026gt; c); //获得用于序列化对象的 Serializer 实现  Serializer\u0026lt;T\u0026gt; getSerializer(Class\u0026lt;T\u0026gt; c); //获得用于反序列化对象的 Deserializer 实现  Deserializer\u0026lt;T\u0026gt; getDeserializer(Class\u0026lt;T\u0026gt; c); }       2）序列化流程  如果需要**使用 Serializer **来执行序列化，一般需要通过 open()方法打开 Serializer，open()方法传入一个 底层的流对象，然后就可以使用serialize()方法序列化对象到底层的流中。最后序列化结束时，通过 close()方法关闭 Serializer。  1 2 3 4 5 6 7 8 9 10  public interface Serializer\u0026lt;T\u0026gt; { //为输出（序列化）对象做准备  void open(OutputStream out) throws IOException; //将对象序列化到底层的流中  void serialize(T t) throws IOException; //序列化结束，清理  void close() throws IOException; }   3）反序列化   如果要使用 deserializer 来执行反序列化，一般需要通过 open()方法打开 deserializer，open()方法传入 一个底层的流对象，然后就可以使用 deserializer()方法反序列化流到对象中。最后反序列化结束时， 通过 close()方法关闭 Serializer。\n1 2 3 4 5 6  public interface deserializer\u0026lt;T\u0026gt; { void open(InputStream in) throws IOException; T deserialize(T t) throws IOException; void close() throws IOException; }     4）Writable接口  Hadoop 的所有可序列化对象都必须实现 这个接口。Writable 接口里有两个方法，一个是 write 方法，将对象写入字节流，另一个是 readFields 方法，从字节流解析出对象。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  public byte[] serialize() throws IOException { IntWritable intWritable = new IntWritable(12); //生成流对象  ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); DataOutputStream dataOutputStream = new DataOutputStream(byteArrayOutputStream); //DataStream封装byteStream  //Serilization the data 序列化对象到流中  intWritable.write(dataOutputStream); byte[] byteArray = byteArrayOutputStream.toByteArray(); dataOutputStream.close(); System.out.println(byteArray); return byteArray; } public int deserialize(byte [] barr) throws IOException { IntWritable intWritable=new IntWritable(); ByteArrayInputStream byteArrayinputStream = new ByteArrayInputStream(barr); DataInputStream dataInputStream= new DataInputStream(byteArrayinputStream); intWritable.readFields(dataInputStream); int data=intWritable.get(); return data; //writable必不可少  }   5）WritableComparable  WritableComparable 接口是可序列化并且可比较的接口，MapReduce 中所有的 key 值类型都必须实现 这个接口。  6）压缩:smile:  GZIP:  压缩率较高，速度较快，hadoop本身就支持，使用方便，支持 hadoop native 库 不支持split 当每个文件压缩之后在 130M 以内的（1 个块大小内），都可以考虑用 gzip 压缩格式   LZO：  压缩速度很快，压缩率合理，支持split，hadoop最流行的压缩格式 压缩率比gzip低，hadoop本身并不支持，需要安装 应用场景：一个很大的文本文件，压缩之后还大于 200M 以上的可以考虑，而且单个文件越大，lzo 优点越越明显。   Snappy:  高速压缩速度和合理的压缩率；支持 hadoop native 库 不支持 split；压缩率比 gzip 要低；hadoop 本身不支持，需要安装；linux 系统下没有对应 的命令。 应用场景：mapreduce中map输出的数据较大，可作为map到reduce的中间数据的压缩格式   bzip2  支持 split；具有很高的压缩率，比 gzip 压缩率都高；hadoop 本身支持，但不支持 native 压缩/解压速度慢；不支持 native。 适合对速度要求不高，但需要较高的压缩率的时候，可以作为 mapreduce 作业的输出格 式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比 较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持 split，而且兼容之 前的应用程序（即应用程序不需要修改）的情况。   CompressionCodec 对流进行压缩和解压缩 每一种编码器(Compressor)/解码器(Decompressor)最后统一的交由编码解码器(CompressionCodec) 来管理    暂时没搞明白 实现原理。:sob: 7）特殊文件处理 :cry:  需求：在处理小文件上低效率和小号磁盘空间的问题 SOlUTION:使用容器：SequenceFile MapFile  SequenceFile：一种二进制文件支持，直接将\u0026lt;key,value\u0026gt;对序列化到文件中。\n 支持压缩，定制基于record或block压缩 简单  在SequenceFile文件中，每一个KV被看作一条记录，所以基于Record的压🔒策略，可以支持三种压缩类型：\n None：对record不压缩 RECORD:仅仅压缩每一个record中的value BLOCK:将一个block中所有records压缩  创建SequenceFile\n1  Remains to be done;   RPC协议 1）定义 远程方法调用，允许计算机程序远程调用另一台计算机的子程序。\n2）Feature  透明性：远程调用其他机器上的程序，对用户来说就像是调用本地方法一样； 高性能：RPC server 能够并发处理多个来自 Client 的请求  3）C/S 模式  Client 端发送一个带有参数的请求信息到 Server Server 接收到这个请求以后，根据发送过来的参数调用相应的程序，然后把自己计算好的结果 发送给 Client 端； Client 端接收到结果后继续运行  Hadoop prc机制 1.RPC设计要求  序列化层：C与S端通信采用了hadoop的序列化类或自定Writable类型 函数调用层：Hadoop RPC通过动态代理以及java反射实现函数调用 网络传输层：采用了基于TCP/IP的socket机制 服务器端框架层：知识盲区  核心框架原理  优点：易于编程，只用关系业务逻辑  良好拓展性：可以动态增加服务器，解决计算资源不够的问题 高容错性：任何一台机器挂掉，可以将任务转移到其他节点 适合海量数据的计算（TB/PB) 几千台服务器共同计算   不擅长实时计算（mysql这样的擅长） 不擅长流式计算（Sparkstreaming flink） 不擅长DAG有向无环图计算 spark  1）MapReduce阶段（引用图） 2）MapReduce进程 一个完整的MapReduce程序在分布式运行时有三类实例进程\n MrAppMaster:负责整个程序的过程调度和状态调度 MapTask：负责整个Map阶段的整个数据处理流程 ReduceTask：负责整个Reduce阶段的整个数据处理流程  3）MapReduce的编程规范   Map阶段\n 用户自定义的Mapper需要继承自己的父类 Mapper的输入数据是KV对的形式（类型可以自定义） Mapper中的业务逻辑实现在map方法中 Map的输出数据也是KV对的形式（同样可以自定义类型） Map方法对每一个\u0026lt;K,V\u0026gt;调用一次（一行一行的调用，多次使用map）    Reduce阶段\n 继承父类 reducer的输入数据类型对应mapper的输出类型 Reducer的业务逻辑写在reduce阶段 ReduceTask进程对每一组k的\u0026lt;k,v\u0026gt;带哦用一次reduce方法（多少K就进行多少次reduce）     Driver：相当于Yarn集群的客户端，用于提交我们整个正序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象\n  MapReduce 1.输入的数据InputFormat数据输入 Map切片大小设置为block大小时，效率最高\n 1）一个job的Map阶段并行度由客户端在提交Job时的切片数决定（切了多少片，多少个Maptask 注意每一个文件需要单独切） 2）每一个Split切片分配一个MapTask并行实例处理 3）默认情况，切片大小=BlockSize 4）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片  Job提交流程源码分析\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  waitForCompletion() submit(); //1 建立连接 \tconnect(); //1)创建提交Job的代理 \tnew Cluster(getConfiguration()); //(1) 判断是本地运行环境还是yarn运行环境 \tinitialize(jobTrackAddr,conf); //2 提交Job \tsubmitter.submitJobInternal(Job.this,cluster) // 1）创建给集群提交数据的Stag路径  Path jobStagingArea = JobsubmissionFiles.getStagingDir(cluster,conf) //2) 获取jobid 并创建job路径  JobID jobId = submitClient.getNewJobID(); //3)如果是集群模式 拷贝jar包到集群 \tcopyAndConfigureFiles(job,submitJobDir) rUploader.uploadFiles(job,jobsubmitDir) //4) 计算切片，生成切片规划文件  //5) 向stag路径写XML配置文件  //6) 提交JOB，返回提交状态   a.FileInputFormat切片源码解析 1 2 3 4 5 6 7 8 9 10 11  1)程序先找到数据存储的目录 2)遍历处理目录下的每一个文件 3)遍历第一个文件ss.txt a)获取文件大小fs.sizeOf(ss.ttx) b)计算切片大小 computeSplitSize(Math.max(minSize,Math,min(maxSize,blockSize)))=blocksize=128M c)默认情况下切片大小都是blocksize d)开始切，形成第一个切片：ss.txt-0:128M 第二个切片ss.txt-128:256 e)将切片信息写到一个切片规划文件中 f)整个切片的核心过程再getSplit()方法完成 g)InputSplit只记录了**切片的元数据信息**,比如起始位置、长度以及所在的节点列表 4)提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask的个数   TextInputFormat是FileInputFormat的实现类 \u0026lt;Longwritable,Text\u0026gt;\n键是存储改行再整个文件中的起始字节偏移量，V是这行的内容\n KeyValueFormat \u0026lt;Text , Text\u0026gt; 第0位 KEY 后面是 Value\nb.CombineTextInputFormat切片机制 CombineTextInputFormat处理方式是一次读取多个文件，把所有文件集合再一起处理.\n应用场景：用于小文件过多的场景，因为再默认框架中\n2.Shuffle  map方法之后。reduce方法之前的过程都成为Shuffle   a.Partitioner   注意：ReduceTask默认为1时，不会走默认的patitioner那个类，而是会运行那个一个内部类使得只有0这个分区\n  问题：要求将统计结果按照条件输出到不同为你暗中（分区），归属地不同省份输出到不同文件中（分区）\n  默认分区是根据key的hashcode对Reduce\u0026rsquo;Task个数取模得到的。用户没法控制哪个Key存储到那个分区\n  自定义Partitoner步骤\n  1）自定义类继承Partitioner，重写getPartitioner方法\n  1 2 3 4 5 6 7 8  public class www extends Partitioner\u0026lt;Text,FlowBean\u0026gt;{ @Override public int getPartitioner(Text key,FlowBean value,int numpartitions) //控制分区代码业务逻辑  ........ return partition; }     2）在job驱动中，设置自定义Partioner\n  1  job.setPartitionerClass(CustomPartitioner.class)     3）自定义partitioner后，需要根据逻辑设置相应数量的ReduceTask\n  4）如果reduceTask数量大于getPartitioner的结果数，则会产生几个空的\n  5）如果reduceTask数量大于1 小于getPartitioner的结果数，则有一部分分区数据无法安防，报错\n  如果reduceTask的数量为一，不管MapTask输出多少个分区的文件，结果都交由一个reduceTask，也就是产生一个结果文件。\n    b.全排序 FlowBean需要实现WritableComparable接口重写compareTo()方法\nc.二次排序 d.区内排序 e.Combiner 会对Key相同的进行合并，并且在MapTask处理一部分合并会提高效率（注意：不是所有都适用）\n Combiner不属于Mapper和Reducer combiner组件的父类是Reducer Combiner和Reducer的区别在于运行的位置：前者在MapTask运行 自定义：基于wordCount的案例 自定义Combiner  3.输出数据OutputFormat  核心方法：recordWriter决定写出类型 自定义outputFormat：  过滤输出日志，包含atguigu的网站输出到\u0026hellip;，不包含\u0026hellip;.的输出到\u0026hellip;. 创建一个类LogRecordWriter继承RecordWriter  (a) 创建两个文件的输出流：aout，bout (b)符合a条件的输出到aout ，其他的输出到bout (c) 要将自定的输出格式组件设置到job中      4.Join 5. ETL 6.总结 7.Yarn在Mapreduce中的作用 ","date":"2022-02-16T00:42:52+08:00","image":"https://yuxin-zh.github.io/AllForOne/p/hadoop-learning/b2_hu0d8ab1c30cfa3ec811729f914159dc22_220525_120x120_fill_q75_box_smart1.jpg","permalink":"https://yuxin-zh.github.io/AllForOne/p/hadoop-learning/","title":"Hadoop Learning"},{"content":"[TOC]\nGit 1.版本控制系统（VCS) 1.1 基本概念 ...行内代码  版本控制系统（VCS）最基本的功能就是版本控制。而所谓版本控制，意思就是在文件的修改历史中保存修改历史，让i方便对文件的i修改工作。 我们常用的主流文本编辑器的undo功能其实就是版本控制。 但是VCS和文本编辑器的撤销功能相比，有一个很重要的区别就是：  对于程序代码而言：修改的生命周期很长，如果采用每次改动自动保存的形式来保存修改历史，将会导致改动历史非常频繁和无章可循。所以和文本编辑器的撤销功能不同，VCS保存修改历史，使用的是主动提交改动的机制。    2.分布式版本控制系统(DVCS) 2.1 工作模型  分布式（DVCS）与VCS的区别在于，分布式VCS除了中央仓库之外，还有本地仓库：团队的每一个成员的机器上都有一份本地昂库，这个仓库包含了所有版本历史，换句话说在这种工作模型内，你是和本地仓库交互。 工作流程  提交代码到本地仓库 在服务器上创建一个中央仓库，并把1中的提交推送到服务器的中央仓库。 其他工作人员将中央仓库的所有内容克隆到本地，拥有了各自的本地仓库，此时进行并行开发 在职过后的开发过程中，么个人都会独立负责开发一个功能，在这个功能的开发过程中，每个人都会把它的每一步改动提交到本地仓库（由于本地提交毋须立即推送到中央仓库，所以提交的内容不一定要是一个完整的功能额模块，而可以是某个步骤） 当完成了某个功能的开发时，可以把与该功能相关的所有提交 从本地仓库推送到中央仓库 而每次有人把新的提交推送到中央仓库是，另外的人就可以选择把这些提交同步到自己的机器上，并把他们和自己的本地代码合并    2.2优缺点分析  😵优点：①大多数操作可以在本地运行，速度更快。②由于可以提交到本地，因此可以分步提交代码，把代码提交做的更细，而不是一个提交包含很多代码，难以review也难以回溯。 🤠缺点：由于每一个旧机器都需要有完整的本地仓库，所以初次获取代码需要获取项目比较费时②本地占用的存储比中央式高  3.使用Git管理代码 3.1 前期准备  Github创建远程仓库 （.gitignore设置项目类型，是git仓库中的一个特俗的文本文件，记录了你不希望提交到仓库的目录和文件的名称或类型 点击右边的Clone or downloda，然后把仓库的Clone地址复制到截切版 在你喜欢的任意一个位置，打开Git Bash,输入Git clone 刚复制的地址（该过程可能会需要你去输入Github的用户名和密码） 克隆完毕后，你的目录中会出现.git的隐藏目录，改了目录就是你的本地仓库，你的所有把那本信息都会存在这里。而.git所在的目录成为工作目录。 一些git的基本指令在这里就不详细介绍了，可以查阅Git官方文档  4.Git基本工作模型  由于git在工作时，必须保证自己的本地仓库与中央仓库保持一致，因此，当你第一次从中央仓库拉去代码后，你的同事又push了一次，那么当你想要把自己的commits提交上去时，就需要先拉取同事的代码到本地。😧 当push时出现冲突：  在现实的团队开发中，全队时同时并行开发的，所以必然会出现当一人push代码时，中央仓库已经被其他同事先一步push了的情况。 那这种情况下，当我们像上面介绍的那样使用git pull指令拉取代码时，他并不会像之前那样直接结束，而是会出现一个输入提交信息的界面，这是因为pull操作返现不仅远程仓库有本地每天有的commits，本地也有远端仓库不具备的commits，它就会把远端和本地独有的commit合并，自动生成一个新的commit 另一种情况，当出现冲突的部分是你和你的同事对某一个文件的某一处进行了不同的修改，Git就无法直接处理了，关于这点会在之后的文章中介绍🦄    5.Head、master、branch的使用讲解 👇👇👇👇👇👇👇👇👇👇👇👇\nWhen Typed \u0026ldquo;git log\u0026rdquo;\n  第一行的commit后面的括号里的 HEAD-\u0026gt; main，是只想这个commit的引用。在Git操作中，经常会需要对指定Commit进行操作，而每一个Commit都会有它唯一的指定方式——它的SHA-1校验和，也就是每个commit中那串黄色的字符。由于SHA-1重复的概率极低，因此你可以使用它来指代某一个commit，也可以只是用他的前几位来指代。\n  Head ：当前的Commit的引用：也就是说当前工作目录所对应的Commit。All in all,Commit在哪里，Head就在哪里。、\n  Branch：是Git中的另一个引用，Head除了可以指向commit，还可以指branch；当他指向branch时，会通过这个branch来简介地指向某个commit；Besides:person_frowning:当Head在提交时自动向前移动时，它会带着它所指向地Branch一起移动。\n  如上图 HEAD-\u0026gt;main,main就是当前分支的名字。\n    1  当你输入Git commit,Head就会带着这个分支，移向下一个commit     你可以通过git log指令 对这个逻辑进行验证，这里就不详细说明了\n    Master：默认branch\n  一般而言，master是Git的默认主分支，它具备一些特点：\n  当你新建仓库时，是不存在任何commit的，但在它创建第一个commit时，会把master指向它，并把Head指向master :grin:\n  当某人使用git clone时，除了从远程仓库把.git这个仓库目录下栽到工作目录中，还会checkout（签出，后面讲解） master\n  类似于这个过程\n    branch的通俗解释：\n  表面上看，branch是一个指向commit的引用，但是你也可以把它理解为从初始 commit 到branch所指向的commit之间的所有Commits的一个串\n  branch相关指令\n  1 2 3 4 5  创建branch： git branch +分支名 切换branch： git checkout +分支名 git checkout -b 名称是这步操作合并执行 删除branch： git branch -d 分支名     branch 创建 切换 删除的流程\n      两个分支出现分叉 :pensive:\n  Notice :imp:Head指向的branch不可以删除；删除branch只是删除这个引用，并不会删除任何的commit；没有被合并到master过的branch在删除时会失败，担心删除错误，但是若想强制删除 把d大写即可\n      6. Push的本质   之前讲的：😳Push指令所做的事是把你的本地提交上传到中央仓库去，用本地的内容覆盖掉远端的内容，其实这个说法是不准确的🐷。\n  实质上，Push所做的事是：把当前branch的位置上传到远端仓库，并把它的路径上的commits一并上传。🙏比如，当前我的本地仓库有一个分支叫master,它超前了远程仓库两个提交（如下图所示）；另外还有一个新建的branch叫feature1，远程仓库还没有记载过，具体情况如下图：🤥\n   此时当我执行git push,就会把master的最新位置更新到远端，并把它的路径上的5,6两个·commit`上传，示例如下：    可以注意到此时远程仓库与master分支的状态同步，但是与feature1的状态不同步，此时可以切换到feature1分支，然后同样的 操作就可以实现将commit 4提交到远程。🦉\n  具体的操作步骤如下：\n1 2  git checkout feature1 git push origin feature1     可以注意到，这里的git push比之前多了两个参数：origin``feature1,其中orgin是远程仓库的别名，是你在git clone时Git默认的名称，feature1则是远程仓库中目标branch的名称。\n  Notice：在Git 2.0版本，git push只能上传从远端clone或git pull下来的分支,而你本地自己创建的分支，在提交时需要手动指定目标仓库和目标分支。\n  🧐好吧，其实你也可以通过git config指令来设置push.deafault的值来改变push的行为逻辑；如果有兴趣，可以点击阅读 Git-Config\n  Okay，当你将Feature1推送到远程仓库时，你会发现，远程仓库的Head并没有和本地一样指向feature1，这是因为Push并不会上传本地的HeAD的指向，而是仅仅只上传当前branc·的指向。⚙实际上，远程仓库的Head永远指向它的默认分支（即master），并随着该默认分支的移动而移动。👳👳👳\n  7.merge：合并commits   Actually，pull的内部操作其实时先fetch即把远程仓库拉取到本地，在使用merge来把远端仓库的新的commits合并到本地，接下来会详细解释下什么是merge。\n  先来段官方解释：从目标commit和当前commit（Head所指向的commit）分叉的位置起，把目标commit的路径上所有commit的内容一并应用到当前commit,然后自动生成一个新的commit.\n  emm 🙃如图所示\n当你此时执行git merge branch1,Git会把5和6这两个commit一起和4合并，并生成一个新的提交，并体哦啊转到信息提交填写界面：\n在信息提交完成之后，merge过程就结束了。\n   Merge的特殊情况处理\n  Conflict Happend 😭\n常规的merge在执行合并操作的时候，具备一定的自动和合并能力：比如当一个分支对A文件进行了修改，另一个分支对B文件进行了修改，那么合并的结果就是A和B都被修改；如果两个分支都修改了同一个文件的不同位置，那么合并后就是两处修改都保存，那么问题来了，当你修改的是同一个文件的同一位置，merge操作会怎么处理呢，这种情况我们成为 Conflict\n假设我们呢正处于冲突的情况：\n1  git merge feature1   提示信息会说你需要把冲突解决后提交\n  解决冲突\n当你打开重现冲突的文件时，你会发现虽然Git没有帮你自动完成Merge但是它对文件还是做了一些工作的，也就是将分支冲突的内容放在了一起，并且标出了它们的出处，Like this：\n你只需要删除掉你不想保留的分支的修改，并删除Git添加的那三行\u0026lt;\u0026lt;\u0026lt; ===``\u0026gt;\u0026gt;\u0026gt;的辅助文字，save and quit，就解决了冲突👬。当然也有一些辅助公寓用于解决冲突，可以自行搜索。\n  解决完冲突就可以手动提交了：git add 出现冲突的文件名 然后 git commit。\n😱可以看到，被冲突阻断的merge，在手动commit时依然会自动填写提交信息，这是因为在冲突发生后，Git仓库处于一个【冲突待解决】的状态，在这种状态下，Git会自动添加提交信息。\n  当然如果你不知道这个冲突该怎么解决，所以你决定放弃处理，也需要执行一次merge --avort来手动取消它，这样你的Git仓库会回到merge之前的状态。\n    HEAD领先于目标commit😓\n  这时merge并不会创建一个新的commit来进行合并操作，在这种情况下，Git什么野不会处理，是一个空操作。\n  Head落后于目标commit👊\n这种操作会让Head直接移动到，目标分支所指向的commit，这种操作有一个专有操作，叫做\u0026quot;fast-forward\u0026quot; 。\n    8.Feature Branching  Feature Branching时目前最流行的gGit工作流，它的核心内容可以归纳为以下两点：  任何新的功能或bug修复全都新建一个branch来写 brnach写完后，合并到master（默认的主分支），然后删掉这个branch    9.About \u0026lsquo;add\u0026rsquo;   add的基本用法\ngit add fileName\n还可以使用git add .来把当前工作目录下的所有改动全部放进暂存区\n其实，add添加的时文件改动，而不是文件名\n    查看git保存的历史记录🆑\n1 2 3 4 5 6 7  git log --查看历史记录 git log -p --可以看到每一个commit的每一行改动 git log -stat --查看简要统计，适用于大概看以下改动内容 git show +某一个commit的引用 --看任意一个commit git diff -staged --显示你即将提交的内容，换句话说，就是如果你立即输入`git commit`，你将提交什么 该指令中staged==cached     10. 用rebase代替merge的工作   Rebase：把你指定的commit以及它所在的commit串，以指定的目标commit为基础，一次重新提交一遍，具体过程如下图：\n可以看出，通过 rebase，5 和 6 两条 commits 把基础点从 2 换成了 4 ；另外，在 rebase 之后，记得切回 master 再 merge 一下，把 master 移到最新的 commit\n  Notice🎃：为了避免和远端仓库发生冲突，一般不要从 master 向其他 branch 执行 rebase 操作。🎃\n   11.Git应用实例 11.1 刚提交的代码，写错了怎么办？🙃   一种解决方法是再新写一个关于修改这个错误的commit\n  另一种则是通过：commit -—amend；在提交时，如果加上 --amend 参数，Git 不会在当前 commit 上增加 commit，而是会把当前 commit 里的内容和暂存区（stageing area）里的内容合并起来后创建一个新的 commit，用这个新的 commit 把当前 commit 替换掉🎉\n1 2  git add 修改后的文件 git commit --amend     11.2错误的不是最新的提交，而是倒数第二个怎么办😥   这时候就可以通过 rebase -i 来找指定当前分支要rebase的commit链中的每一个commit是否需要修改。因此你可以做如下操作\n1  git log   1  git rebase -i HEAD^^   1 2 3 4 5 6  说明：在 Git 中，有两个「偏移符号」： ^ 和 ~。 ^ 的用法：在 commit 的后面加一个或多个 ^ 号，可以把 commit 往回偏移，偏移的数量是 ^ 的数量。例如：master^ 表示 master 指向的 commit 之前的那个 commit； HEAD^^ 表示 HEAD 所指向的 commit 往前数两个 commit。 ~ 的用法：在 commit 的后面加上 ~ 号和一个数，可以把 commit 往回偏移，偏移的数量是 ~ 号后面的数。例如：HEAD~5 表示 HEAD 指向的 commit往前数 5 个 commit。   执行后，会出现如下界面：\n  这个编辑界面的最顶部，列出了将要「被 rebase」的所有 commits，也就是倒数第二个 commit 「增加常见笑声集合」和最新的 commit「增加常见哭声集合」。需要注意，这个排列是正序的，旧的 commit 会排在上面，新的排在下面。\n  你的目标是修改倒数第二个 commit，也就是上面的那个「增加常见笑声集合」，所以你需要把它的操作指令从 pick 改成 edit 。 edit 的意思是「应用这个 commit，然后停下来等待继续修正」\n把 pick 修改成 edit 后，就可以退出编辑界面了：此时rebase 过程已经停在了第二个 commit 的位置，那么现在你就可以去修改你想修改的内容了。\n  git add 修改后的文件 git commit --amend\n  修复完成后 使用 git rebase --continue来继续rebase过程，把后面的commit 直接应用上去\n  可能看文字描述有点不能理解，👌上图：\ngit rebase -i HEAD^^\ngit rebase --continue\n    11.3 甚至都不想修改，删除算了   丢弃最新的提交使用：git reset --hard HEAD^\n1  HEAD^`表示你要恢复到哪个`commit`。因为你要撤销最新的一个`commit`，所以你需要恢复到它的父`commit`，也就是`HEAD^    Reset的实质：其实是重置HEAD以及它所指向的branch的位置的，\n 这样就需要提下git reset --hard和git reset --soft的区别了：前者是在重置 HEAD 和 branch 的同时，重置工作目录里的内容；后者则是会在重置 HEAD 和 branch 时，保留工作目录和暂存区中的内容，并把重置 HEAD 所带来的新的差异放进暂存区。 例如 hard：你新修改了一个文件，但是并没有提交，使用了hard指令，此时你的工作目录里的新改动也消失了，并与reset切换到的commit保持同样的内容；soft则是会保留工作目录和暂存区的内容。    丢弃的不是最新的提交 ：\n那就变基到你想删除的提交之前的那个COMMIT：git rebase -i HEAD~?，然后再提示信息界面删除那个提交即可。\n  关于rebase -i进行说明\nrebase -i其实可以理解为把当前head所指向的commit之前的某个提交作为基点，从该基点开始，对后面的当前分支的commit进行提交，但是它会先给个提示信息界面用于你去选择对基点之后的commit进行操作\n  方便的撤销：git onto 🤘\n1  git rebase --onto 目标commit 起点commit 终点commitv Notice:起点不包含再rebase中     11.4 代码已经push上去了🔐，并合并到master   在这种情况下，你就需要做一个把这行代码还原回来的提交：revert\ngit revert HEAD^这行代码会增加一条新的提交，它的内容和倒数第二个commit是相反的，从而达到撤销的目的。⚠\n  12.CheckOut本质  前面说到checkout可以用来切换 branch；但是实质上，它的功能其实是：**签出指定的commit**🤪，通俗的讲，就是把 HEAD 指向指定的 branch，然后签出这个 branch 所对应的 commit 的工作目录。所以同样的，checkout 的目标也可以不是 branch，而直接指定某个 commit✏ checkout和reset都可以切换head的位置，但是reset同时移动HEAD和它所指向的branch，而checkout只移动head  13. 关于.gitignore  在 Git 中有一个特殊的文本文件：.gitignore。这个文本文件记录了所有你希望被 Git 忽略的目录和文件。具体的规则可查阅 👉 💾 👈  14.使用git常出现的错误(REAMINS TO BE DONE) ","date":"2022-02-15T00:01:02+08:00","image":"https://yuxin-zh.github.io/AllForOne/p/git/b1_hu17068975d7620e962a90e7eb4393cc6e_69230_120x120_fill_q75_box_smart1.jpg","permalink":"https://yuxin-zh.github.io/AllForOne/p/git/","title":"Git"},{"content":"wwwwww\n","date":"2020-09-09T00:00:00Z","image":"https://yuxin-zh.github.io/AllForOne/p/test-chinese/12_hu45a5e3ad5e058da6a00650ed8fd40bea_15530_120x120_fill_q75_box_smart1.jpg","permalink":"https://yuxin-zh.github.io/AllForOne/p/test-chinese/","title":"Test"}]